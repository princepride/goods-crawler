{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18576089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取并解码 HTML 内容 (使用编码: utf-8)。\n",
      "\n",
      "正在使用 BeautifulSoup 解析提取的 HTML...\n",
      "查找所有 class 为 'product__message-title_area' 的 div 元素...\n",
      "找到了 48 个匹配的元素：\n",
      "正在关闭 WebDriver...\n",
      "WebDriver 已关闭。\n",
      "\n",
      "正在将提取的 48 条产品数据保存到 Excel 文件: coles_data.xlsx\n",
      "Excel 文件 'coles_data.xlsx' 保存成功。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import email\n",
    "import email.policy # 使用推荐的策略来解析\n",
    "from bs4 import BeautifulSoup\n",
    "import os # 用于检查文件是否存在\n",
    "import pandas as pd\n",
    "\n",
    "user_data_dir = r\"C:\\Users\\wangz\\AppData\\Local\\Google\\Chrome\\User Data\" # <-- 把 '你的用户名' 换成你的实际 Windows 用户名\n",
    "profile_directory = \"Default\"  # 或者 \"Profile 1\", \"Profile 2\" 等，取决于 chrome://version 显示的\n",
    "\n",
    "target_class = 'product__message-title_area' # 你要查找的 class 名称\n",
    "html_content = None # 用来存储提取出来的 HTML 文本\n",
    "# -----------\n",
    "excel_file_name = 'coles_data.xlsx'\n",
    "product_data = []\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "chrome_options.add_argument(f\"user-data-dir={user_data_dir}\")\n",
    "chrome_options.add_argument(f\"profile-directory={profile_directory}\")\n",
    "\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument(\"--disable-extensions\") # 注意：这个选项可能会阻止加载你本地配置中的扩展，如果需要扩展运行，可以注释掉这行\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "\n",
    "driver = None # 初始化 driver 变量\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    main_page_url = \"https://www.coles.com.au\"\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    pageNumber = 2\n",
    "    special_url = f'https://www.coles.com.au/on-special?filter_Special=halfprice&page={pageNumber}'\n",
    "    driver.get(special_url)\n",
    "    time.sleep(5)\n",
    "    result = driver.execute_cdp_cmd('Page.captureSnapshot', {'format': 'mhtml'})\n",
    "    msg = email.message_from_string(result['data'])\n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "        # 主要的HTML内容通常是 'text/html'\n",
    "        if content_type == 'text/html':\n",
    "            # 获取内容负载并解码 (decode=True 会处理 base64 或 quoted-printable 编码)\n",
    "            payload_bytes = part.get_payload(decode=True)\n",
    "            # 尝试从 header 获取编码，否则默认 utf-8\n",
    "            charset = part.get_content_charset() or 'utf-8'\n",
    "            try:\n",
    "                # 将字节解码为字符串\n",
    "                html_content = payload_bytes.decode(charset)\n",
    "                print(f\"成功提取并解码 HTML 内容 (使用编码: {charset})。\")\n",
    "                break # 找到主要的 HTML 部分就停止\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"警告：使用声明的编码 '{charset}' 解码失败，尝试使用 'utf-8' 作为备用...\")\n",
    "                try:\n",
    "                    html_content = payload_bytes.decode('utf-8')\n",
    "                    print(\"成功使用 'utf-8' 备用编码解码。\")\n",
    "                    break\n",
    "                except Exception as decode_err:\n",
    "                    print(f\"使用备用编码解码也失败: {decode_err}\")\n",
    "                    # 这里可以选择继续查找其他 text/html 部分，或者直接放弃\n",
    "                    html_content = None # 重置以防部分解码\n",
    "                    continue # 继续查找下一个部分\n",
    "\n",
    "    # 4. 如果成功提取了 HTML 内容，则使用 BeautifulSoup 解析\n",
    "    if html_content:\n",
    "        print(f\"\\n正在使用 BeautifulSoup 解析提取的 HTML...\")\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        print(f\"查找所有 class 为 '{target_class}' 的 div 元素...\")\n",
    "        # 5. 查找所有符合条件的 div 元素\n",
    "        elements = soup.find_all('div', class_=target_class)\n",
    "\n",
    "        # 6. 打印找到的元素内容\n",
    "        if elements:\n",
    "            print(f\"找到了 {len(elements)} 个匹配的元素：\")\n",
    "            for i, element in enumerate(elements):\n",
    "                product_info = {\n",
    "                    '产品名称': \"N/A\",\n",
    "                    '原价': \"N/A\",\n",
    "                    '现价': \"N/A\",\n",
    "                    '单位价格': \"N/A\"\n",
    "                }\n",
    "                product_info['产品名称'] = element.find(\"h2\", class_=\"product__title\").contents[0]\n",
    "                product_info['原价'] = element.find(\"span\", class_=\"price__was\").contents[0].replace(\" | Was \", \"\")\n",
    "                product_info['现价'] = element.find(\"span\", class_=\"price__value\").contents[0]\n",
    "                product_info['单位价格'] = element.find(\"div\", class_=\"price__calculation_method\").contents[0]\n",
    "                product_data.append(product_info)\n",
    "        else:\n",
    "            print(f\"在 HTML 内容中未找到任何 class 为 '{target_class}' 的 div 元素。\")\n",
    "    else:\n",
    "        print(\"错误：未能在 MHTML 文件中找到有效的 HTML 内容部分。\")\n",
    "except Exception as e:\n",
    "    print(f\"在处理页面或保存 MHTML 时出错: {e}\")\n",
    "    # import traceback; traceback.print_exc() # 取消注释查看详细错误栈\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        print(\"正在关闭 WebDriver...\")\n",
    "        driver.quit()\n",
    "        print(\"WebDriver 已关闭。\")\n",
    "    else:\n",
    "        print(\"WebDriver 未成功初始化，无需关闭。\")\n",
    "\n",
    "if product_data:\n",
    "    print(f\"\\n正在将提取的 {len(product_data)} 条产品数据保存到 Excel 文件: {excel_file_name}\")\n",
    "    try:\n",
    "        # 将字典列表转换为 pandas DataFrame\n",
    "        df = pd.DataFrame(product_data)\n",
    "        # 可以指定列的顺序\n",
    "        df = df[['产品名称', '现价', '原价', '单位价格']]\n",
    "        # 将 DataFrame 写入 Excel 文件，不包含索引列\n",
    "        df.to_excel(excel_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"Excel 文件 '{excel_file_name}' 保存成功。\")\n",
    "    except ImportError:\n",
    "            print(\"错误: 需要安装 'pandas' 和 'openpyxl' 库来保存 Excel 文件。请运行: pip install pandas openpyxl\")\n",
    "    except Exception as ex:\n",
    "            print(f\"保存 Excel 文件时出错: {ex}\")\n",
    "else:\n",
    "    print(\"没有提取到任何产品数据，未创建 Excel 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac453872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取并解码第 1 页的 HTML 内容 (使用编码: utf-8)。\n",
      "总页数已找到: 25\n",
      "在第 1 页找到了 52 个匹配的元素：\n",
      "成功提取并解码第 2 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 2 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 3 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 3 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 4 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 4 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 5 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 5 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 6 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 6 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 7 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 7 页找到了 56 个匹配的元素：\n",
      "成功提取并解码第 8 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 8 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 9 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 9 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 10 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 10 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 11 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 11 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 12 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 12 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 13 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 13 页找到了 56 个匹配的元素：\n",
      "成功提取并解码第 14 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 14 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 15 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 15 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 16 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 16 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 17 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 17 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 18 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 18 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 19 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 19 页找到了 56 个匹配的元素：\n",
      "成功提取并解码第 20 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 20 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 21 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 21 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 22 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 22 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 23 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 23 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 24 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 24 页找到了 48 个匹配的元素：\n",
      "成功提取并解码第 25 页的 HTML 内容 (使用编码: utf-8)。\n",
      "在第 25 页找到了 21 个匹配的元素：\n",
      "正在关闭 WebDriver...\n",
      "WebDriver 已关闭。\n",
      "\n",
      "正在将提取的 1201 条产品数据保存到 Excel 文件: coles_data.xlsx\n",
      "Excel 文件 'coles_data.xlsx' 保存成功。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import email\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "user_data_dir = r\"C:\\Users\\wangz\\AppData\\Local\\Google\\Chrome\\User Data\"\n",
    "profile_directory = \"Default\"\n",
    "\n",
    "target_class = 'product__message-title_area'\n",
    "product_data = []\n",
    "excel_file_name = 'coles_data.xlsx'\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"user-data-dir={user_data_dir}\")\n",
    "chrome_options.add_argument(f\"profile-directory={profile_directory}\")\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    main_page_url = \"https://www.coles.com.au\"\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(20)\n",
    "\n",
    "    special_url_base = 'https://www.coles.com.au/on-special?filter_Special=halfprice&page='\n",
    "    current_page = 1\n",
    "    total_pages = 1  # 初始化为 1\n",
    "\n",
    "    while current_page <= total_pages:\n",
    "        special_url = f'{special_url_base}{current_page}'\n",
    "        driver.get(special_url)\n",
    "        time.sleep(5)\n",
    "        result = driver.execute_cdp_cmd('Page.captureSnapshot', {'format': 'mhtml'})\n",
    "        msg = email.message_from_string(result['data'])\n",
    "        html_content = None\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            if content_type == 'text/html':\n",
    "                payload_bytes = part.get_payload(decode=True)\n",
    "                charset = part.get_content_charset() or 'utf-8'\n",
    "                try:\n",
    "                    html_content = payload_bytes.decode(charset)\n",
    "                    print(f\"成功提取并解码第 {current_page} 页的 HTML 内容 (使用编码: {charset})。\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"警告：使用声明的编码 '{charset}' 解码第 {current_page} 页失败，尝试使用 'utf-8' 作为备用...\")\n",
    "                    try:\n",
    "                        html_content = payload_bytes.decode('utf-8')\n",
    "                        print(f\"成功使用 'utf-8' 备用编码解码第 {current_page} 页。\")\n",
    "                        break\n",
    "                    except Exception as decode_err:\n",
    "                        print(f\"使用备用编码解码第 {current_page} 页也失败: {decode_err}\")\n",
    "                        html_content = None\n",
    "                        continue\n",
    "                except Exception as decode_err:\n",
    "                    print(f\"解码第 {current_page} 页时发生其他错误: {decode_err}\")\n",
    "                    html_content = None\n",
    "                    continue\n",
    "\n",
    "        if html_content:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # 在第一页获取总页数\n",
    "            if current_page == 1:\n",
    "                page_buttons = soup.find_all(\"span\", class_=\"MuiButtonBase-root\")\n",
    "                if page_buttons:\n",
    "                    try:\n",
    "                        total_pages_text = page_buttons[-1].get_text(strip=True)\n",
    "                        total_pages = int(total_pages_text)\n",
    "                        print(f\"总页数已找到: {total_pages}\")\n",
    "                    except ValueError:\n",
    "                        print(\"警告：无法将总页数文本转换为整数，假设只有 1 页。\")\n",
    "                        total_pages = 1\n",
    "                    except IndexError:\n",
    "                        print(\"警告：未找到分页按钮，假设只有 1 页。\")\n",
    "                        total_pages = 1\n",
    "                else:\n",
    "                    print(\"警告：未找到分页相关的元素，假设只有 1 页。\")\n",
    "                    total_pages = 1\n",
    "\n",
    "            elements = soup.find_all('div', class_=target_class)\n",
    "\n",
    "            if elements:\n",
    "                print(f\"在第 {current_page} 页找到了 {len(elements)} 个匹配的元素：\")\n",
    "                for element in elements:\n",
    "                    product_info = {\n",
    "                        '产品名称': \"N/A\",\n",
    "                        '原价': \"N/A\",\n",
    "                        '现价': \"N/A\",\n",
    "                        '单位价格': \"N/A\"\n",
    "                    }\n",
    "                    title_element = element.find(\"h2\", class_=\"product__title\")\n",
    "                    was_price_element = element.find(\"span\", class_=\"price__was\")\n",
    "                    now_price_element = element.find(\"span\", class_=\"price__value\")\n",
    "                    unit_price_element = element.find(\"div\", class_=\"price__calculation_method\")\n",
    "\n",
    "                    if title_element and title_element.contents:\n",
    "                        product_info['产品名称'] = title_element.contents[0]\n",
    "                    if was_price_element and was_price_element.contents:\n",
    "                        product_info['原价'] = was_price_element.contents[0].replace(\" | Was \", \"\")\n",
    "                    if now_price_element and now_price_element.contents:\n",
    "                        product_info['现价'] = now_price_element.contents[0]\n",
    "                    if unit_price_element and unit_price_element.contents:\n",
    "                        product_info['单位价格'] = unit_price_element.contents[0]\n",
    "\n",
    "                    product_data.append(product_info)\n",
    "            else:\n",
    "                print(f\"在第 {current_page} 页的 HTML 内容中未找到任何 class 为 '{target_class}' 的 div 元素。\")\n",
    "        else:\n",
    "            print(f\"错误：未能在第 {current_page} 页的 MHTML 文件中找到有效的 HTML 内容部分。\")\n",
    "\n",
    "        current_page += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"在处理页面或保存 MHTML 时出错: {e}\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        print(\"正在关闭 WebDriver...\")\n",
    "        driver.quit()\n",
    "        print(\"WebDriver 已关闭。\")\n",
    "    else:\n",
    "        print(\"WebDriver 未成功初始化，无需关闭。\")\n",
    "\n",
    "if product_data:\n",
    "    print(f\"\\n正在将提取的 {len(product_data)} 条产品数据保存到 Excel 文件: {excel_file_name}\")\n",
    "    try:\n",
    "        df = pd.DataFrame(product_data)\n",
    "        df = df[['产品名称', '现价', '原价', '单位价格']]\n",
    "        df.to_excel(excel_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"Excel 文件 '{excel_file_name}' 保存成功。\")\n",
    "    except ImportError:\n",
    "        print(\"错误: 需要安装 'pandas' 和 'openpyxl' 库来保存 Excel 文件。请运行: pip install pandas openpyxl\")\n",
    "    except Exception as ex:\n",
    "        print(f\"保存 Excel 文件时出错: {ex}\")\n",
    "else:\n",
    "    print(\"没有提取到任何产品数据，未创建 Excel 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38901641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(soup.find_all(\"span\", class_=\"MuiButtonBase-root\")[-1].get_text(strip=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
